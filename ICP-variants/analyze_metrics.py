#!/usr/bin/env python3
"""
ICP Metrics Analysis Script

This script analyzes the metrics data generated by the ICP implementation
and creates visualizations to help understand algorithm performance.
"""

# =============================================================================
# IMPORTS AND DEPENDENCIES
# =============================================================================
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sys
import os
from datetime import datetime

# =============================================================================
# DATA LOADING FUNCTIONS
# =============================================================================
def load_metrics_data(filename='icp_metrics.log'):
    """Load metrics data from CSV file."""
    if not os.path.exists(filename):
        print(f"Error: Metrics file '{filename}' not found.")
        print("Run the ICP program first to generate metrics data.")
        return None
    
    try:
        df = pd.read_csv(filename)
        print(f"Loaded {len(df)} metric entries from {filename}")
        return df
    except Exception as e:
        print(f"Error loading metrics file: {e}")
        return None

# =============================================================================
# CONVERGENCE ANALYSIS FUNCTIONS
# =============================================================================
def analyze_convergence(df):
    """Analyze convergence patterns for each optimizer."""
    print("\n=== Convergence Analysis ===")
    
    # Process each optimizer separately to compare convergence behavior
    optimizers = df['Optimizer'].unique()
    
    for optimizer in optimizers:
        opt_data = df[df['Optimizer'] == optimizer]
        print(f"\n{optimizer}:")
        
        if len(opt_data) > 0:
            # Calculate improvement metrics
            initial_rmse = opt_data.iloc[0]['RMSE']
            final_rmse = opt_data.iloc[-1]['RMSE']
            improvement = ((initial_rmse - final_rmse) / initial_rmse) * 100
            
            # Display convergence statistics
            print(f"  Initial RMSE: {initial_rmse:.6f}")
            print(f"  Final RMSE: {final_rmse:.6f}")
            print(f"  Improvement: {improvement:.2f}%")
            print(f"  Iterations: {len(opt_data)}")
            
            # Determine convergence status based on RMSE stability
            if len(opt_data) > 1:
                last_two_rmse = opt_data.tail(2)['RMSE'].values
                if abs(last_two_rmse[1] - last_two_rmse[0]) < 1e-6:
                    print("  Status: Converged")
                else:
                    print("  Status: Not converged")

# =============================================================================
# OPTIMIZER COMPARISON FUNCTIONS
# =============================================================================
def compare_optimizers(df):
    """Compare performance metrics between different optimizers."""
    print("\n=== Optimizer Comparison ===")
    
    optimizers = df['Optimizer'].unique()
    
    # Aggregate key performance metrics for each optimizer
    comparison_data = []
    for optimizer in optimizers:
        opt_data = df[df['Optimizer'] == optimizer]
        if len(opt_data) > 0:
            # Extract final performance metrics
            final_rmse = opt_data.iloc[-1]['RMSE']
            total_time = opt_data['ComputationTime'].sum()
            avg_correspondences = opt_data['ValidCorrespondences'].mean()
            
            comparison_data.append({
                'Optimizer': optimizer,
                'Final_RMSE': final_rmse,
                'Total_Time': total_time,
                'Avg_Correspondences': avg_correspondences,
                'Iterations': len(opt_data)
            })
    
    # Display comparison table and identify best performers
    if comparison_data:
        comp_df = pd.DataFrame(comparison_data)
        print(comp_df.to_string(index=False))
        
        # Find best optimizer by accuracy (lowest RMSE)
        best_rmse = comp_df.loc[comp_df['Final_RMSE'].idxmin()]
        print(f"\nBest RMSE: {best_rmse['Optimizer']} ({best_rmse['Final_RMSE']:.6f})")
        
        # Find fastest optimizer (lowest total time)
        fastest = comp_df.loc[comp_df['Total_Time'].idxmin()]
        print(f"Fastest: {fastest['Optimizer']} ({fastest['Total_Time']:.3f}s)")

# =============================================================================
# VISUALIZATION FUNCTIONS
# =============================================================================
def create_visualizations(df):
    """Create comprehensive visualizations of the ICP metrics data."""
    print("\n=== Creating Visualizations ===")
    
    # Set up multi-panel figure layout
    plt.style.use('default')
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle('ICP Metrics Analysis', fontsize=16)
    
    # Prepare color schemes for different optimizers
    optimizers = df['Optimizer'].unique()
    colors = plt.cm.Set1(np.linspace(0, 1, len(optimizers)))
    
    # PLOT 1: RMSE Convergence Over Iterations
    ax1 = axes[0, 0]
    colors = plt.cm.Set1(np.linspace(0, 1, len(optimizers)))
    for i, optimizer in enumerate(optimizers):
        opt_data = df[df['Optimizer'] == optimizer]
        ax1.plot(opt_data['Iteration'], opt_data['RMSE'], 
                marker='o', label=optimizer, color=colors[i], linewidth=2, markersize=6)
    ax1.set_xlabel('Iteration')
    ax1.set_ylabel('RMSE')
    ax1.set_title('RMSE vs Iteration')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # PLOT 2: Mean Distance Evolution
    ax2 = axes[0, 1]
    colors = plt.cm.Set1(np.linspace(0, 1, len(optimizers)))
    for i, optimizer in enumerate(optimizers):
        opt_data = df[df['Optimizer'] == optimizer]
        ax2.plot(opt_data['Iteration'], opt_data['MeanDistance'], 
                marker='s', label=optimizer, color=colors[i], linewidth=2, markersize=6)
    ax2.set_xlabel('Iteration')
    ax2.set_ylabel('Mean Distance')
    ax2.set_title('Mean Distance vs Iteration')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # PLOT 3: Valid Correspondences Tracking
    ax3 = axes[0, 2]
    colors = plt.cm.Set3(np.linspace(0, 1, len(optimizers)))
    for i, optimizer in enumerate(optimizers):
        opt_data = df[df['Optimizer'] == optimizer]
        ax3.plot(opt_data['Iteration'], opt_data['ValidCorrespondences'], 
                marker='^', label=optimizer, color=colors[i], linewidth=2, markersize=6)
    ax3.set_xlabel('Iteration')
    ax3.set_ylabel('Valid Correspondences')
    ax3.set_title('Valid Correspondences vs Iteration')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # PLOT 4: Computational Performance Analysis
    ax4 = axes[1, 0]
    colors = plt.cm.Accent(np.linspace(0, 1, len(optimizers)))
    for i, optimizer in enumerate(optimizers):
        opt_data = df[df['Optimizer'] == optimizer]
        ax4.plot(opt_data['Iteration'], opt_data['ComputationTime'], 
                marker='d', label=optimizer, color=colors[i], linewidth=2, markersize=6)
    ax4.set_xlabel('Iteration')
    ax4.set_ylabel('Computation Time (s)')
    ax4.set_title('Computation Time vs Iteration')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # PLOT 5: Error Distribution Analysis
    ax5 = axes[1, 1]
    colors = plt.cm.Dark2(np.linspace(0, 1, len(optimizers)))
    for i, optimizer in enumerate(optimizers):
        opt_data = df[df['Optimizer'] == optimizer]
        ax5.plot(opt_data['Iteration'], opt_data['StdDeviation'], 
                marker='*', label=optimizer, color=colors[i], linewidth=2, markersize=8)
    ax5.set_xlabel('Iteration')
    ax5.set_ylabel('Standard Deviation')
    ax5.set_title('Standard Deviation vs Iteration')
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # PLOT 6: Final Performance Comparison Bar Chart
    ax6 = axes[1, 2]
    colors = plt.cm.Paired(np.linspace(0, 1, len(optimizers)))
    final_rmse_data = []
    labels = []
    bar_colors = []
    for i, optimizer in enumerate(optimizers):
        opt_data = df[df['Optimizer'] == optimizer]
        if len(opt_data) > 0:
            final_rmse_data.append(opt_data.iloc[-1]['RMSE'])
            labels.append(optimizer)
            bar_colors.append(colors[i])
    
    if final_rmse_data:
        ax6.bar(labels, final_rmse_data, color=bar_colors, alpha=0.8, edgecolor='black', linewidth=1)
        ax6.set_ylabel('Final RMSE')
        ax6.set_title('Final RMSE Comparison')
        ax6.tick_params(axis='x', rotation=45)
    
    # Save and display the complete visualization
    plt.tight_layout()
    plt.savefig('icp_metrics_analysis.png', dpi=300, bbox_inches='tight')
    print("Visualization saved as 'icp_metrics_analysis.png'")
    
    # Attempt to show plot in interactive environments
    try:
        plt.show()
    except:
        pass

# =============================================================================
# REPORT GENERATION FUNCTIONS
# =============================================================================
def generate_report(df):
    """Generate a comprehensive text-based analysis report."""
    print("\n=== Generating Analysis Report ===")
    
    # Create timestamped report filename
    report_filename = f"icp_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    
    with open(report_filename, 'w') as f:
        # Report header and metadata
        f.write("ICP Metrics Analysis Report\n")
        f.write("=" * 50 + "\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # High-level summary statistics
        f.write("SUMMARY STATISTICS\n")
        f.write("-" * 20 + "\n")
        f.write(f"Total metric entries: {len(df)}\n")
        f.write(f"Optimizers tested: {', '.join(df['Optimizer'].unique())}\n")
        f.write(f"Date range: {df['Timestamp'].min()} to {df['Timestamp'].max()}\n\n")
        
        # Detailed per-optimizer performance analysis
        for optimizer in df['Optimizer'].unique():
            opt_data = df[df['Optimizer'] == optimizer]
            f.write(f"OPTIMIZER: {optimizer}\n")
            f.write("-" * 30 + "\n")
            f.write(f"Total iterations: {len(opt_data)}\n")
            f.write(f"Initial RMSE: {opt_data.iloc[0]['RMSE']:.6f}\n")
            f.write(f"Final RMSE: {opt_data.iloc[-1]['RMSE']:.6f}\n")
            f.write(f"Total computation time: {opt_data['ComputationTime'].sum():.3f}s\n")
            f.write(f"Average correspondences: {opt_data['ValidCorrespondences'].mean():.1f}\n")
            f.write(f"Best RMSE: {opt_data['RMSE'].min():.6f}\n")
            f.write(f"Worst RMSE: {opt_data['RMSE'].max():.6f}\n\n")
        
        # Performance-based recommendations
        f.write("RECOMMENDATIONS\n")
        f.write("-" * 15 + "\n")
        
        # Identify best performers for different criteria
        best_rmse_opt = df.loc[df.groupby('Optimizer')['RMSE'].idxmin()]['Optimizer'].iloc[0]
        fastest_opt = df.groupby('Optimizer')['ComputationTime'].sum().idxmin()
        
        f.write(f"Best accuracy: {best_rmse_opt}\n")
        f.write(f"Fastest execution: {fastest_opt}\n")
        
        # Provide strategic recommendations
        if best_rmse_opt == fastest_opt:
            f.write("The fastest optimizer also provides the best accuracy.\n")
        else:
            f.write("Consider the trade-off between speed and accuracy.\n")
    
    print(f"Report saved as '{report_filename}'")

# =============================================================================
# MAIN EXECUTION FUNCTION
# =============================================================================
def main():
    """Main orchestration function for the complete analysis workflow."""
    print("ICP Metrics Analysis Tool")
    print("=" * 30)
    
    # STEP 1: Load and validate metrics data
    df = load_metrics_data()
    if df is None:
        return
    
    # STEP 2: Perform convergence analysis
    analyze_convergence(df)
    
    # STEP 3: Compare optimizer performance
    compare_optimizers(df)
    
    # STEP 4: Generate comprehensive visualizations
    create_visualizations(df)
    
    # STEP 5: Create detailed analysis report
    generate_report(df)
    
    print("\nAnalysis complete!")

# =============================================================================
# SCRIPT ENTRY POINT
# =============================================================================
if __name__ == "__main__":
    main() 