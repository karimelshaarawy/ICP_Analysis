#!/usr/bin/env python3
"""
ICP Metrics Analysis Script

This script analyzes the metrics data generated by the ICP implementation
and creates visualizations to help understand algorithm performance.
"""

import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import sys
import os
from datetime import datetime

def load_metrics_data(filename='icp_metrics.log'):
    """Load metrics data from CSV file."""
    if not os.path.exists(filename):
        print(f"Error: Metrics file '{filename}' not found.")
        print("Run the ICP program first to generate metrics data.")
        return None
    
    try:
        df = pd.read_csv(filename)
        print(f"Loaded {len(df)} metric entries from {filename}")
        return df
    except Exception as e:
        print(f"Error loading metrics file: {e}")
        return None

def analyze_convergence(df):
    """Analyze convergence patterns."""
    print("\n=== Convergence Analysis ===")
    
    # Group by optimizer and iteration
    optimizers = df['Optimizer'].unique()
    
    for optimizer in optimizers:
        opt_data = df[df['Optimizer'] == optimizer]
        print(f"\n{optimizer}:")
        
        if len(opt_data) > 0:
            initial_rmse = opt_data.iloc[0]['RMSE']
            final_rmse = opt_data.iloc[-1]['RMSE']
            improvement = ((initial_rmse - final_rmse) / initial_rmse) * 100
            
            print(f"  Initial RMSE: {initial_rmse:.6f}")
            print(f"  Final RMSE: {final_rmse:.6f}")
            print(f"  Improvement: {improvement:.2f}%")
            print(f"  Iterations: {len(opt_data)}")
            
            # Check if converged
            if len(opt_data) > 1:
                last_two_rmse = opt_data.tail(2)['RMSE'].values
                if abs(last_two_rmse[1] - last_two_rmse[0]) < 1e-6:
                    print("  Status: Converged")
                else:
                    print("  Status: Not converged")

def compare_optimizers(df):
    """Compare performance between different optimizers."""
    print("\n=== Optimizer Comparison ===")
    
    optimizers = df['Optimizer'].unique()
    
    comparison_data = []
    for optimizer in optimizers:
        opt_data = df[df['Optimizer'] == optimizer]
        if len(opt_data) > 0:
            final_rmse = opt_data.iloc[-1]['RMSE']
            total_time = opt_data['ComputationTime'].sum()
            avg_correspondences = opt_data['ValidCorrespondences'].mean()
            
            comparison_data.append({
                'Optimizer': optimizer,
                'Final_RMSE': final_rmse,
                'Total_Time': total_time,
                'Avg_Correspondences': avg_correspondences,
                'Iterations': len(opt_data)
            })
    
    if comparison_data:
        comp_df = pd.DataFrame(comparison_data)
        print(comp_df.to_string(index=False))
        
        # Find best optimizer by RMSE
        best_rmse = comp_df.loc[comp_df['Final_RMSE'].idxmin()]
        print(f"\nBest RMSE: {best_rmse['Optimizer']} ({best_rmse['Final_RMSE']:.6f})")
        
        # Find fastest optimizer
        fastest = comp_df.loc[comp_df['Total_Time'].idxmin()]
        print(f"Fastest: {fastest['Optimizer']} ({fastest['Total_Time']:.3f}s)")

def create_visualizations(df):
    """Create various visualizations of the metrics data."""
    print("\n=== Creating Visualizations ===")
    
    # Set up the plotting style
    plt.style.use('default')
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle('ICP Metrics Analysis', fontsize=16)
    
    optimizers = df['Optimizer'].unique()
    colors = plt.cm.Set1(np.linspace(0, 1, len(optimizers)))
    
    # 1. RMSE vs Iteration
    ax1 = axes[0, 0]
    for i, optimizer in enumerate(optimizers):
        opt_data = df[df['Optimizer'] == optimizer]
        ax1.plot(opt_data['Iteration'], opt_data['RMSE'], 
                marker='o', label=optimizer, color=colors[i])
    ax1.set_xlabel('Iteration')
    ax1.set_ylabel('RMSE')
    ax1.set_title('RMSE vs Iteration')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. Mean Distance vs Iteration
    ax2 = axes[0, 1]
    for i, optimizer in enumerate(optimizers):
        opt_data = df[df['Optimizer'] == optimizer]
        ax2.plot(opt_data['Iteration'], opt_data['MeanDistance'], 
                marker='s', label=optimizer, color=colors[i])
    ax2.set_xlabel('Iteration')
    ax2.set_ylabel('Mean Distance')
    ax2.set_title('Mean Distance vs Iteration')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Valid Correspondences vs Iteration
    ax3 = axes[0, 2]
    for i, optimizer in enumerate(optimizers):
        opt_data = df[df['Optimizer'] == optimizer]
        ax3.plot(opt_data['Iteration'], opt_data['ValidCorrespondences'], 
                marker='^', label=optimizer, color=colors[i])
    ax3.set_xlabel('Iteration')
    ax3.set_ylabel('Valid Correspondences')
    ax3.set_title('Valid Correspondences vs Iteration')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. Computation Time vs Iteration
    ax4 = axes[1, 0]
    for i, optimizer in enumerate(optimizers):
        opt_data = df[df['Optimizer'] == optimizer]
        ax4.plot(opt_data['Iteration'], opt_data['ComputationTime'], 
                marker='d', label=optimizer, color=colors[i])
    ax4.set_xlabel('Iteration')
    ax4.set_ylabel('Computation Time (s)')
    ax4.set_title('Computation Time vs Iteration')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # 5. Standard Deviation vs Iteration
    ax5 = axes[1, 1]
    for i, optimizer in enumerate(optimizers):
        opt_data = df[df['Optimizer'] == optimizer]
        ax5.plot(opt_data['Iteration'], opt_data['StdDeviation'], 
                marker='*', label=optimizer, color=colors[i])
    ax5.set_xlabel('Iteration')
    ax5.set_ylabel('Standard Deviation')
    ax5.set_title('Standard Deviation vs Iteration')
    ax5.legend()
    ax5.grid(True, alpha=0.3)
    
    # 6. Box plot of final RMSE values
    ax6 = axes[1, 2]
    final_rmse_data = []
    labels = []
    for optimizer in optimizers:
        opt_data = df[df['Optimizer'] == optimizer]
        if len(opt_data) > 0:
            final_rmse_data.append(opt_data.iloc[-1]['RMSE'])
            labels.append(optimizer)
    
    if final_rmse_data:
        ax6.bar(labels, final_rmse_data, color=colors[:len(labels)])
        ax6.set_ylabel('Final RMSE')
        ax6.set_title('Final RMSE Comparison')
        ax6.tick_params(axis='x', rotation=45)
    
    plt.tight_layout()
    plt.savefig('icp_metrics_analysis.png', dpi=300, bbox_inches='tight')
    print("Visualization saved as 'icp_metrics_analysis.png'")
    
    # Show the plot if in interactive mode
    try:
        plt.show()
    except:
        pass

def generate_report(df):
    """Generate a comprehensive analysis report."""
    print("\n=== Generating Analysis Report ===")
    
    report_filename = f"icp_analysis_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
    
    with open(report_filename, 'w') as f:
        f.write("ICP Metrics Analysis Report\n")
        f.write("=" * 50 + "\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # Summary statistics
        f.write("SUMMARY STATISTICS\n")
        f.write("-" * 20 + "\n")
        f.write(f"Total metric entries: {len(df)}\n")
        f.write(f"Optimizers tested: {', '.join(df['Optimizer'].unique())}\n")
        f.write(f"Date range: {df['Timestamp'].min()} to {df['Timestamp'].max()}\n\n")
        
        # Per-optimizer analysis
        for optimizer in df['Optimizer'].unique():
            opt_data = df[df['Optimizer'] == optimizer]
            f.write(f"OPTIMIZER: {optimizer}\n")
            f.write("-" * 30 + "\n")
            f.write(f"Total iterations: {len(opt_data)}\n")
            f.write(f"Initial RMSE: {opt_data.iloc[0]['RMSE']:.6f}\n")
            f.write(f"Final RMSE: {opt_data.iloc[-1]['RMSE']:.6f}\n")
            f.write(f"Total computation time: {opt_data['ComputationTime'].sum():.3f}s\n")
            f.write(f"Average correspondences: {opt_data['ValidCorrespondences'].mean():.1f}\n")
            f.write(f"Best RMSE: {opt_data['RMSE'].min():.6f}\n")
            f.write(f"Worst RMSE: {opt_data['RMSE'].max():.6f}\n\n")
        
        # Recommendations
        f.write("RECOMMENDATIONS\n")
        f.write("-" * 15 + "\n")
        
        best_rmse_opt = df.loc[df.groupby('Optimizer')['RMSE'].idxmin()]['Optimizer'].iloc[0]
        fastest_opt = df.groupby('Optimizer')['ComputationTime'].sum().idxmin()
        
        f.write(f"Best accuracy: {best_rmse_opt}\n")
        f.write(f"Fastest execution: {fastest_opt}\n")
        
        if best_rmse_opt == fastest_opt:
            f.write("The fastest optimizer also provides the best accuracy.\n")
        else:
            f.write("Consider the trade-off between speed and accuracy.\n")
    
    print(f"Report saved as '{report_filename}'")

def main():
    """Main function to run the analysis."""
    print("ICP Metrics Analysis Tool")
    print("=" * 30)
    
    # Load data
    df = load_metrics_data()
    if df is None:
        return
    
    # Perform analysis
    analyze_convergence(df)
    compare_optimizers(df)
    
    # Create visualizations
    create_visualizations(df)
    
    # Generate report
    generate_report(df)
    
    print("\nAnalysis complete!")

if __name__ == "__main__":
    main() 